{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 2c. Loading large datasets progressively with the tf.data.Dataset </h1>\n",
    "\n",
    "In this notebook, we continue reading the same small dataset, but refactor our ML pipeline in two small, but significant, ways:\n",
    "<ol>\n",
    "<li> Refactor the input to read data from disk progressively.\n",
    "<li> Refactor the feature creation so that it is not one-to-one with inputs.\n",
    "</ol>\n",
    "<br/>\n",
    "The Pandas function in the previous notebook first read the whole data into memory -- on a large dataset, this won't be an option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Exercise\n",
    "\n",
    "Create a neural network that is capable of finding the volume of a cylinder given the radius of its base (r) and its height (h). Assume that the radius and height of the cylinder are both in the range 0.5 to 2.0. Unlike in the challenge exercise for b_estimator.ipynb, assume that your measurements of r, h and V are all rounded off to the nearest 0.1. Simulate the necessary training dataset. This time, you will need a lot more data to get a good predictor.\n",
    "\n",
    "Now modify the \"noise\" so that instead of just rounding off the value, there is up to a 10% error (uniformly distributed) in the measurement followed by rounding off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py3env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "def make_some_noise(array, error=0.1):\n",
    "  return np.random.uniform(array*(1-error), array*(1+error))\n",
    "\n",
    "def initialise_data(n_rows, value_range=(5,20), random_seed=696, error=0.1):\n",
    "  \"\"\" Generate random dataset for model training \"\"\"\n",
    "  # Seed\n",
    "  np.random.seed(random_seed)\n",
    "  \n",
    "  # Generate r and h\n",
    "  frame = pd.DataFrame({\n",
    "    'r': np.random.uniform(value_range[0], value_range[1], n_rows),\n",
    "    'h': np.random.uniform(value_range[0], value_range[1], n_rows),\n",
    "  })\n",
    "  \n",
    "  # Compute v\n",
    "  frame['v'] = np.pi*frame['r']**2 * frame['h']\n",
    "  \n",
    "  # Add noise to measurements\n",
    "  frame[['r', 'h']] = frame[['r', 'h']].apply(make_some_noise, error=error)\n",
    "  \n",
    "  # Round value to the nearest 0.1\n",
    "  frame = frame.apply(np.round, decimals=1)\n",
    "  \n",
    "  return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h</th>\n",
       "      <th>r</th>\n",
       "      <th>v</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.6</td>\n",
       "      <td>16.3</td>\n",
       "      <td>12056.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.2</td>\n",
       "      <td>12.4</td>\n",
       "      <td>10810.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.5</td>\n",
       "      <td>6.4</td>\n",
       "      <td>1404.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.4</td>\n",
       "      <td>6.3</td>\n",
       "      <td>1771.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.4</td>\n",
       "      <td>17.7</td>\n",
       "      <td>9628.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      h     r        v\n",
       "0  14.6  16.3  12056.6\n",
       "1  20.2  12.4  10810.9\n",
       "2   9.5   6.4   1404.1\n",
       "3  13.4   6.3   1771.5\n",
       "4  11.4  17.7   9628.8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "initialise_data(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate datasets\n",
    "train_file = './cyl-train.csv'\n",
    "val_file = './cyl-val.csv'\n",
    "\n",
    "initialise_data(50000).to_csv(train_file, header=False, index=False)\n",
    "initialise_data(5000, random_seed=969).to_csv(val_file, header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tfUtils:\n",
    "  def __init__(self, train_file, val_file, num_epochs=50, feature_cols=['h','r'], label='v', batch_size=64, queue_capacity=1000, defaults=[[1.1], [1.1], [1.1]]):\n",
    "    self.num_epochs = num_epochs\n",
    "    self.label = label\n",
    "    self.batch_size = batch_size\n",
    "    self.queue_capacity = queue_capacity\n",
    "    self.features = feature_cols\n",
    "    self.defaults = defaults\n",
    "    self.train_file = train_file\n",
    "    self.val_file = val_file\n",
    "    \n",
    "    self.columns = feature_cols + list(label)\n",
    "    \n",
    "  def decode_csv(self, row):\n",
    "    columns = tf.decode_csv(row, record_defaults=self.defaults)\n",
    "    \n",
    "    # Separate features and label\n",
    "    features = dict(zip(self.columns, columns))    \n",
    "    label = features.pop(self.label)\n",
    "    \n",
    "    return features, label\n",
    "\n",
    "  def read_dataset(self, filename, mode, batch_size=1024, header=False):\n",
    "    # Create list of file names that match \"glob\" pattern (i.e. data_file_*.csv)\n",
    "    filenames = tf.data.Dataset.list_files(filename, shuffle=False)\n",
    "    \n",
    "    # Skip the first row if there's header\n",
    "    if header:\n",
    "      filenames = filenames.skip(1)\n",
    "    \n",
    "    # Read lines from text files\n",
    "    textlines = filenames.flat_map(tf.data.TextLineDataset)\n",
    "    \n",
    "    # Parse text lines as comma-separated values (CSV)\n",
    "    dataset = textlines.map(self.decode_csv)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        num_epochs = None\n",
    "        dataset = dataset.shuffle(buffer_size=10 * batch_size, seed=2)\n",
    "    else:\n",
    "        num_epochs = 1 # end-of-input after this\n",
    "\n",
    "    dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "\n",
    "    return dataset\n",
    "  \n",
    "  def get_feature_cols(self):\n",
    "    return [tf.feature_column.numeric_column(k) for k in self.features]\n",
    "\n",
    "  def get_train_input_fn(self):\n",
    "    return self.read_dataset(self.train_file, mode=tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "  def get_eval_input_fn(self):\n",
    "    return self.read_dataset(self.val_file, mode=tf.estimator.ModeKeys.EVAL)\n",
    "  \n",
    "  def compute_rmse(self, model, file):\n",
    "    metrics = model.evaluate(input_fn=self.get_eval_input_fn)\n",
    "    print('RMSE = ', np.sqrt(metrics['average_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise\n",
    "tf_utils = tfUtils(train_file, val_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpuw1l16ym\n",
      "INFO:tensorflow:Using config: {'_service': None, '_keep_checkpoint_every_n_hours': 10000, '_task_type': 'worker', '_save_checkpoints_steps': None, '_tf_random_seed': None, '_task_id': 0, '_master': '', '_train_distribute': None, '_evaluation_master': '', '_global_id_in_cluster': 0, '_session_config': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f83984fff98>, '_is_chief': True, '_model_dir': '/tmp/tmpuw1l16ym', '_num_worker_replicas': 1, '_log_step_count_steps': 100, '_save_summary_steps': 100, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_save_checkpoints_secs': 600}\n"
     ]
    }
   ],
   "source": [
    "# Set logging settings\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# Initialise DNN Regressor\n",
    "model = tf.estimator.DNNRegressor(\n",
    "          hidden_units=[3,2,1],\n",
    "          feature_columns=tf_utils.get_feature_cols()\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpuw1l16ym/model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 76150220000.0\n",
      "INFO:tensorflow:global_step/sec: 35.3381\n",
      "INFO:tensorflow:step = 101, loss = 72862010000.0 (2.836 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.0956\n",
      "INFO:tensorflow:step = 201, loss = 80288840000.0 (2.849 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.0102\n",
      "INFO:tensorflow:step = 301, loss = 78698410000.0 (2.858 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.5196\n",
      "INFO:tensorflow:step = 401, loss = 70392570000.0 (2.814 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.4902\n",
      "INFO:tensorflow:step = 501, loss = 76875860000.0 (2.818 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.6313\n",
      "INFO:tensorflow:step = 601, loss = 75832730000.0 (2.887 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.7037\n",
      "INFO:tensorflow:step = 701, loss = 76487210000.0 (2.879 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.5164\n",
      "INFO:tensorflow:step = 801, loss = 72219165000.0 (2.819 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.28\n",
      "INFO:tensorflow:step = 901, loss = 76965310000.0 (2.833 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.0576\n",
      "INFO:tensorflow:step = 1001, loss = 77757560000.0 (2.852 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.1487\n",
      "INFO:tensorflow:step = 1101, loss = 75907220000.0 (2.847 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.3962\n",
      "INFO:tensorflow:step = 1201, loss = 72809620000.0 (2.824 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.8867\n",
      "INFO:tensorflow:step = 1301, loss = 72858550000.0 (2.866 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.6615\n",
      "INFO:tensorflow:step = 1401, loss = 72782690000.0 (2.804 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.4536\n",
      "INFO:tensorflow:step = 1501, loss = 78018970000.0 (2.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 34.8797\n",
      "INFO:tensorflow:step = 1601, loss = 71886670000.0 (2.867 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.6801\n",
      "INFO:tensorflow:step = 1701, loss = 75108460000.0 (2.805 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.008\n",
      "INFO:tensorflow:step = 1801, loss = 75767530000.0 (2.773 sec)\n",
      "INFO:tensorflow:global_step/sec: 35.8382\n",
      "INFO:tensorflow:step = 1901, loss = 75592745000.0 (2.791 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /tmp/tmpuw1l16ym/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 71192720000.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNRegressor at 0x7f83984ff080>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "model.train(input_fn=tf_utils.get_train_input_fn, steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-02-13-16:39:32\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpuw1l16ym/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-02-13-16:39:33\n",
      "INFO:tensorflow:Saving dict for global step 2000: average_loss = 71455350.0, global_step = 2000, loss = 71455360000.0\n",
      "RMSE =  8453.127\n"
     ]
    }
   ],
   "source": [
    "tf_utils.compute_rmse(model, val_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
